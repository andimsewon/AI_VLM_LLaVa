# LLaVa-1.5-7B 모델 미세 조정 README

이 프로젝트는 **LLaVa-1.5-7B** 모델을 **HuggingFace**와 **TRL**을 사용하여 미세 조정(Fine-Tuning)하는 과정을 다룹니다. 이 모델은 텍스트와 이미지를 조합하여 질문에 대한 답변을 생성할 수 있는 시각-언어 모델(Vision-Language Model, VLM)입니다.

## 프로젝트 개요

본 프로젝트의 목표는 **쇼핑몰 CCTV 이미지**를 활용해 사람 수를 추정하는 **이미지 기반 질문 응답 모델**을 만드는 것입니다. **LoRA (Low-Rank Adaptation)** 및 **4-bit 양자화** 기법을 사용하여 **메모리 효율적**이고 **리소스 절약적인 방식**으로 대형 모델을 미세 조정합니다.

## 기능 및 주요 특징
- **4-bit 양자화**를 사용하여 LLaVa-1.5-7B 모델을 로드해 메모리 사용량을 줄임.
- 사용자와 AI 간의 대화 형식을 위한 **채팅 템플릿** 사용.
- **Hugging Face Dataset**을 활용하여 학습 데이터를 준비하고, 학습 및 평가 데이터셋으로 분할.
- **Tensorboard**로 학습 과정을 모니터링.
- 학습된 모델을 **Hugging Face Hub**에 업로드하여 다른 사람과 공유.

## 요구 사항

- **Python 3.7+**
- **PyTorch**
- **Transformers (v4.39+)**
- **TRL (v0.8.3+)**
- **PEFT**
- **BitsAndBytes**
- **Google Colab** (권장)

필수 라이브러리 설치:
```bash
pip install -U "transformers>=4.39.0"
pip install peft bitsandbytes
pip install -U "trl>=0.8.3"
```

## 데이터 준비

### 데이터 설명
- **이미지 데이터 (`frames` 폴더)**: 쇼핑몰 CCTV에서 캡처한 이미지입니다.
- **텍스트 데이터 (`messages.txt`)**: 모델과 사용자 간의 예시 대화 데이터입니다.
- **CSV 메타데이터 (`labels.csv`)**: 각 이미지에 포함된 사람 수에 대한 정보를 포함하고 있습니다.

데이터를 로드한 후, **Hugging Face Dataset** 형식으로 변환하여 학습 및 평가에 사용할 준비를 합니다.

### 데이터셋 구축 과정
- **메시지 템플릿**을 생성하여 모델이 이해할 수 있는 대화 형식으로 데이터를 구성합니다.
- 각 이미지에 대해 관련된 질문과 답변을 포함한 데이터셋을 구성하여 학습 데이터와 평가 데이터로 **80:20 비율**로 나눕니다.
- 학습에 적합한 형태로 데이터를 저장하여 나중에 불러올 수 있도록 합니다.

## 학습 과정

### 모델 로드 및 양자화
- 모델은 Hugging Face Hub에서 로드되며, **4-bit 양자화**를 적용하여 **메모리 사용량**을 절감합니다.
- **LoRA** 설정을 통해 모델의 일부를 효율적으로 미세 조정합니다.

### 학습 설정
- 학습은 다음과 같은 주요 하이퍼파라미터로 수행됩니다:
  - **학습률**: `1.4e-5`
  - **배치 크기**: `8`
  - **에폭 수**: `1`
  - **Gradient Accumulation Steps**: `1`
  - **Gradient Checkpointing**: 활성화하여 메모리 절약
  - **Tensorboard**: 학습 진행 상황을 시각적으로 모니터링하기 위해 사용
- 학습이 완료된 모델은 **Hugging Face Hub**에 업로드됩니다.

### 데이터 Collator
- **텍스트**와 **이미지** 데이터를 동시에 처리할 수 있는 데이터 Collator 함수를 정의합니다. 이 함수는 배치를 구성할 때 사용됩니다.

### 성능 평가
- 학습된 모델의 성능은 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1 점수를 계산하여 평가합니다. 이를 통해 모델이 사람 수를 추정하는 작업을 얼마나 잘 수행하는지 알 수 있습니다.

## 모델 사용 방법

1. 이 저장소를 클론합니다:
   ```bash
   git clone https://github.com/yourusername/fine-tune-llava.git
   ```
2. 필요한 패키지를 설치합니다.
3. **Google Colab**에서 `fine_tune_VLM_LlaVa.ipynb` 파일을 엽니다.
4. 데이터를 액세스하기 위해 Google Drive를 마운트합니다.
5. 노트북의 지침에 따라 학습 과정을 진행합니다.

## 결론 및 향후 작업

### 결론
- 이 프로젝트는 **LLaVa-1.5-7B**와 같은 대형 시각-언어 모델을 효율적으로 미세 조정하여 이미지와 텍스트 데이터를 결합한 질문 응답 시스템을 성공적으로 구축하는 과정을 보여줍니다.
- **LoRA**와 **4-bit 양자화**를 사용하여 일반적인 하드웨어에서도 실행 가능한 모델로 만들었습니다.

### 향후 작업
- 더 다양한 데이터셋을 사용하여 학습을 확장하고, **모델의 일반화 능력**을 개선합니다.
- **추론 속도 최적화**를 통해 실시간 응답 시스템으로의 적용을 고려합니다.
- 다른 **효율적인 미세 조정 기법**을 실험하여 모델 성능을 개선합니다.

## 기여 방법

프로젝트에 기여하고 싶다면 다음 절차를 따라주세요:
1. 이 저장소를 포크합니다.
2. 새로운 기능이나 버그 수정을 위한 브랜치를 만듭니다:
   ```bash
   git checkout -b feature/새로운-기능-이름
   ```
3. 변경 사항을 커밋합니다:
   ```bash
   git commit -m "새로운 기능 추가 설명"
   ```
4. 브랜치에 푸시합니다:
   ```bash
   git push origin feature/새로운-기능-이름
   ```
5. 풀 리퀘스트를 만듭니다.

기여는 언제나 환영입니다! 새로운 기능 제안이나 버그 리포트를 통해도 기여할 수 있습니다.

## 라이선스

이 프로젝트는 MIT 라이선스 하에 라이선스가 부여되었습니다. 자세한 내용은 `LICENSE` 파일을 참조하세요.

## 감사의 말씀
- Hugging Face 팀의 오픈 소스 모델과 도구들에 감사드립니다.
- 원본 **Colab** 설정 스크립트를 제공해 주신 **@mrm8488**님께 감사드립니다.

기여를 원하시면 풀 리퀘스트 제출 또는 이슈 보고를 통해 참여해 주세요!

